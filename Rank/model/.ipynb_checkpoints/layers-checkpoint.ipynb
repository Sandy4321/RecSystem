{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, BatchNormalization, Activation, Dropout, Dense, Embedding\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import zeros, glorot_normal, glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, use_bias=False, reg_l2=0.0, **kwargs):\n",
    "        self.use_bias = use_bias\n",
    "        self.reg_l2 = reg_l2\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.dense = Dense(1, use_bias=False, kernel_regularizer=l2(self.reg_l2))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name='linear_bias', shape=(1,), initializer=zeros())\n",
    "        super(Linear, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # [batch_size * feat_num * emb_size, batch_size * feat_num]\n",
    "        sparsez_emb, dense_value = inputs\n",
    "        linear_logit = tf.reduce_sum(sparsez_emb, axis=1) + self.dense(dense_value)\n",
    "        if self.use_bias:\n",
    "            linear_logit += self.bias\n",
    "            \n",
    "        return linear_logit\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(FM, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(FM, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # batch_size * feat_num * emb_size\n",
    "        embXvalue = inputs\n",
    "        square_of_sum = tf.square(tf.reduce_sum(embXvalue, axis=1))\n",
    "        sum_of_square = tf.reduce_sum(embXvalue * embXvalue, axis=1)\n",
    "        fm_logit = 0.5 * tf.reduce_sum(square_of_sum - sum_of_square, axis=1, keepdims=True)\n",
    "\n",
    "        return fm_logit\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFMLayer(Layer):\n",
    "    \n",
    "    def __init__(self, attention_factor=4, reg_l2=0.0, dropout_rate=0.0, **kwargs):\n",
    "        self.attention_factor = attention_factor\n",
    "        self.reg_l2 = reg_l2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        super(AFMLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        emb_size = input_shape[-1]\n",
    "        self.attention_W = self.add_weight(name=\"attention_W\", \n",
    "                                          shape=(emb_size, self.attention_factor), \n",
    "                                          initializer=glorot_normal(), \n",
    "                                          regularizer=l2(self.reg_l2))\n",
    "        self.attention_b = self.add_weight(name=\"attention_b\", \n",
    "                                          shape=(self.attention_factor,), \n",
    "                                          initializer=zeros())\n",
    "        self.projection_h = self.add_weight(name=\"projection_h\",\n",
    "                                           shape=(self.attention_factor, 1),\n",
    "                                           initializer=glorot_normal())\n",
    "        self.projection_p = self.add_weight(name=\"projection_p\",\n",
    "                                           shape=(emb_size, 1),\n",
    "                                           initializer=glorot_normal())\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        super(AFMLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        field_size = inputs.shape[1]\n",
    "        vij = []\n",
    "        for i in range(field_size):\n",
    "            for j in range(i+1, field_size):\n",
    "                vij.append(tf.expand_dims(tf.multiply(inputs[:,i], inputs[:,j]), axis=1))\n",
    "                \n",
    "        attention_inputs = tf.concat(vij, axis=1)\n",
    "        attention_temp = tf.nn.relu(tf.nn.bias_add(\n",
    "            tf.tensordot(attention_inputs, self.attention_W, axes=(-1,0)),   self.attention_b))\n",
    "        attention_score = tf.nn.softmax(tf.tensordot(attention_temp, self.projection_h, axes=(-1,0)), axis=1)\n",
    "        attention_output = tf.reduce_sum(tf.multiply(attention_inputs, attention_score), axis=1)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        \n",
    "        afm_logit = tf.tensordot(attention_output, self.projection_p, axes=(-1, 0))\n",
    "        return afm_logit\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiInteractionPooling(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(BiInteractionPooling, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(BiInteractionPooling, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # batch_size * feat_num * emb_size\n",
    "        embXvalue = inputs\n",
    "        square_of_sum = tf.square(tf.reduce_sum(embXvalue, axis=1))\n",
    "        sum_of_square = tf.reduce_sum(embXvalue * embXvalue, axis=1)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "\n",
    "        return cross_term\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMLayer(Layer):\n",
    "    \n",
    "    def __init__(self, field_size, feat_size, emb_size=4, reg_l2=0.0, **kwargs):\n",
    "        self.field_size = field_size\n",
    "        self.feat_size = feat_size\n",
    "        self.emb_size = emb_size\n",
    "        self.reg_l2 = reg_l2\n",
    "        super(FFMLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.V = {i: Embedding(self.feat_size, self.emb_size, embeddings_regularizer=l2(self.reg_l2)) \n",
    "                  for i in range(self.field_size)}\n",
    "        super(FFMLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # batch_size * feat_num\n",
    "        feat_index = inputs\n",
    "        ffm_logit = []\n",
    "        for i in range(self.field_size):\n",
    "            for j in range(i+1, self.field_size):\n",
    "                vi_fj = self.V[j](feat_index[:, i])\n",
    "                vj_fi = self.V[i](feat_index[:, j])\n",
    "                w = tf.reduce_sum(tf.multiply(vi_fj, vj_fi), axis=-1, keepdims=True)\n",
    "                ffm_logit.append(w)\n",
    "\n",
    "        ffm_logit = tf.reduce_sum(ffm_logit, axis=0)\n",
    "        return ffm_logit\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(Layer):\n",
    "    \n",
    "    def __init__(self, hidden_units=[128, 128], reg_l2=0.0, dropout_rate=0.0, use_bn=False, **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.reg_l2 = reg_l2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        super(DNN, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_size = input_shape[-1]\n",
    "        hidden_units = [input_size] + self.hidden_units\n",
    "        self.kernels = [self.add_weight(name='kernel'+str(i),\n",
    "                                        shape=(hidden_units[i], hidden_units[i+1]),\n",
    "                                        initializer=glorot_uniform(),\n",
    "                                        regularizer=l2(self.reg_l2),\n",
    "                                       ) for i in range(len(self.hidden_units))]\n",
    "        self.bias = [self.add_weight(name='bias'+str(i),\n",
    "                                        shape=(self.hidden_units[i],),\n",
    "                                        initializer=zeros(),\n",
    "                                       ) for i in range(len(self.hidden_units))]\n",
    "        if self.use_bn:\n",
    "            self.bn_layers = [BatchNormalization() for _ in range(len(self.hidden_units))]\n",
    "        self.activation_layers = [Activation('relu') for _ in range(len(self.hidden_units))]\n",
    "        self.dropout_layers = [Dropout(self.dropout_rate) for _ in range(len(self.hidden_units))]\n",
    "        \n",
    "        super(DNN, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # batch_size * feat_num\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            dnn_output = tf.nn.bias_add(tf.matmul(inputs, self.kernels[i]), self.bias[i])\n",
    "            if self.use_bn:\n",
    "                dnn_output = self.bn_layers[i](dnn_output)\n",
    "            dnn_output = self.activation_layers[i](dnn_output)\n",
    "            dnn_output = self.dropout_layers[i](dnn_output)\n",
    "            inputs = dnn_output\n",
    "            \n",
    "        return dnn_output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = input_shape[:-1] + self.hidden_units[-1]\n",
    "        return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNet(Layer):\n",
    "    \n",
    "    def __init__(self, layer_num=2, reg_l2=0.0, **kwargs):\n",
    "        self.layer_num = layer_num\n",
    "        self.reg_l2 = reg_l2\n",
    "        super(CrossNet, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_size = input_shape[-1]\n",
    "        self.kernels = [self.add_weight(name='kernel'+str(i),\n",
    "                                       shape=(input_size, 1),\n",
    "                                       initializer=glorot_uniform(),\n",
    "                                       regularizer=l2(self.reg_l2),\n",
    "                                       ) for i in range(self.layer_num)]\n",
    "        self.bias = [self.add_weight(name='bias'+str(i),\n",
    "                                    shape=(input_size, 1),\n",
    "                                    initializer=zeros(),\n",
    "                                    ) for i in range(self.layer_num)]\n",
    "        super(CrossNet, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # batch_size * feat_num\n",
    "        x_0 = tf.expand_dims(inputs, axis=-1)\n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            dot = tf.matmul(x_0, tf.matmul(tf.transpose(x_l, perm=[0,2,1]), self.kernels[i]))\n",
    "            x_l = dot + self.bias[i] + x_l\n",
    "        \n",
    "        output = tf.squeeze(x_l, axis=-1)\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIN(Layer):\n",
    "\n",
    "    def __init__(self, layer_size=[128, 128], activation='relu', split_half=True, reg_l2=1e-5, **kwargs):\n",
    "        self.layer_size = layer_size\n",
    "        self.split_half = split_half\n",
    "        self.activation = activation\n",
    "        self.reg_l2 = reg_l2\n",
    "        super(CIN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.field_nums = [int(input_shape[1])]\n",
    "        self.filters = []\n",
    "        self.bias = []\n",
    "        for i, size in enumerate(self.layer_size):\n",
    "\n",
    "            self.filters.append(self.add_weight(name='filter' + str(i),\n",
    "                                                shape=[1, self.field_nums[-1] * self.field_nums[0], size],\n",
    "                                                dtype=tf.float32, \n",
    "                                                initializer=glorot_uniform(),\n",
    "                                                regularizer=l2(self.reg_l2)))\n",
    "\n",
    "            self.bias.append(self.add_weight(name='bias' + str(i), \n",
    "                                             shape=[size], \n",
    "                                             dtype=tf.float32,\n",
    "                                             initializer=zeros()))\n",
    "\n",
    "            if self.split_half:\n",
    "                self.field_nums.append(size // 2)\n",
    "            else:\n",
    "                self.field_nums.append(size)\n",
    "\n",
    "        self.activation_layers = [Activation(self.activation) for _ in self.layer_size]\n",
    "\n",
    "        super(CIN, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        dim = int(inputs.get_shape()[-1])\n",
    "        hidden_nn_layers = [inputs]\n",
    "        final_result = []\n",
    "\n",
    "        split_tensor0 = tf.split(hidden_nn_layers[0], dim * [1], 2)\n",
    "        for idx, layer_size in enumerate(self.layer_size):\n",
    "            split_tensor = tf.split(hidden_nn_layers[-1], dim * [1], 2)\n",
    "\n",
    "            dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)\n",
    "\n",
    "            dot_result_o = tf.reshape(dot_result_m, shape=[dim, -1, self.field_nums[0] * self.field_nums[idx]])\n",
    "\n",
    "            dot_result = tf.transpose(dot_result_o, perm=[1, 0, 2])\n",
    "\n",
    "            curr_out = tf.nn.conv1d(dot_result, filters=self.filters[idx], stride=1, padding='VALID')\n",
    "\n",
    "            curr_out = tf.nn.bias_add(curr_out, self.bias[idx])\n",
    "\n",
    "            curr_out = self.activation_layers[idx](curr_out)\n",
    "\n",
    "            curr_out = tf.transpose(curr_out, perm=[0, 2, 1])\n",
    "\n",
    "            if self.split_half:\n",
    "                if idx != len(self.layer_size) - 1:\n",
    "                    next_hidden, direct_connect = tf.split(\n",
    "                        curr_out, 2 * [layer_size // 2], 1)\n",
    "                else:\n",
    "                    direct_connect = curr_out\n",
    "                    next_hidden = 0\n",
    "            else:\n",
    "                direct_connect = curr_out\n",
    "                next_hidden = curr_out\n",
    "\n",
    "            final_result.append(direct_connect)\n",
    "            hidden_nn_layers.append(next_hidden)\n",
    "\n",
    "        result = tf.concat(final_result, axis=1)\n",
    "        result = tf.reduce_sum(result, -1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.split_half:\n",
    "            featuremap_num = sum(self.layer_size[:-1]) // 2 + self.layer_size[-1]\n",
    "        else:\n",
    "            featuremap_num = sum(self.layer_size)\n",
    "        return (None, featuremap_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
